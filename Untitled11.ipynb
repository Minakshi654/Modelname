{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMibILU+tJxb0jjo7iDGKkB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Minakshi654/Modelname/blob/main/Untitled11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "xV8fSVknxezN",
        "outputId": "fec8f7cb-4efc-42db-dec9-cf20f5af622f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.00029112299381302953\n",
            "RMSE: 0.01706232674089409\n",
            "Accuracy: 99.99%\n",
            "F1 Score: 0.5142857142857143\n",
            "Correlation: 0.987341431349786\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length of values (120005) does not match length of index (120036)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-95915ed53436>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Add status column to test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Status'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Overflow'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Normal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4089\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4090\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4091\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4093\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4298\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4299\u001b[0m         \"\"\"\n\u001b[0;32m-> 4300\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4302\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5038\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5039\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5040\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \"\"\"\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (120005) does not match length of index (120036)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Intervals.csv', parse_dates=['Interval_start'], dayfirst=True)\n",
        "\n",
        "# Ensure the data is sorted by date\n",
        "df = df.sort_values('Interval_start')\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "df['Max_total'] = scaler.fit_transform(df[['Max_total']])\n",
        "\n",
        "# Prepare the data for Random Forest\n",
        "def create_dataset(data, time_step=1):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data)-time_step-1):\n",
        "        a = data[i:(i+time_step), 0]\n",
        "        X.append(a)\n",
        "        Y.append(data[i + time_step, 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(len(df) * 0.8)\n",
        "train, test = df[:train_size], df[train_size:]\n",
        "\n",
        "# Reshape the data\n",
        "time_step = 30  # 30 minutes\n",
        "X_train, y_train = create_dataset(train[['Max_total']].values, time_step)\n",
        "X_test, y_test = create_dataset(test[['Max_total']].values, time_step)\n",
        "\n",
        "# Build the Random Forest model\n",
        "model = RandomForestRegressor(n_estimators=10)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predict = model.predict(X_train)\n",
        "test_predict = model.predict(X_test)\n",
        "\n",
        "# Inverse transform to get actual values\n",
        "train_predict = scaler.inverse_transform(train_predict.reshape(-1, 1))\n",
        "y_train = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
        "test_predict = scaler.inverse_transform(test_predict.reshape(-1, 1))\n",
        "y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test, test_predict)\n",
        "rmse = np.sqrt(mse)\n",
        "accuracy = accuracy_score(np.where(y_test >= 1, 'Overflow', 'Normal'), np.where(test_predict >= 1, 'Overflow', 'Normal'))\n",
        "f1 = f1_score(np.where(y_test >= 1, 'Overflow', 'Normal'), np.where(test_predict >= 1, 'Overflow', 'Normal'), pos_label='Overflow')\n",
        "correlation = np.corrcoef(y_test[:, 0], test_predict[:, 0])[0, 1]\n",
        "\n",
        "# Print metrics\n",
        "print(f'MSE: {mse}')\n",
        "print(f'RMSE: {rmse}')\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Correlation: {correlation}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "source": [
        "# Add status column to test set\n",
        "test = test.reset_index(drop=True)\n",
        "test['Predicted'] = pd.Series(test_predict.flatten()) # Convert test_predict to a Series to ensure matching length\n",
        "test['Status'] = np.where(test['Predicted'] >= 1, 'Overflow', 'Normal')\n",
        "num_overflows = (test['Predicted'] >= 1).sum()\n",
        "print(f'Number of overflows: {num_overflows}')\n",
        "# Save the results\n",
        "test.to_csv('Predicted_Intervals.csv', index=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf-7ybzT4-1D",
        "outputId": "6b412c0d-3522-4f52-b078-b987442dc4cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of overflows: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Intervals.csv', parse_dates=['Interval_start'], dayfirst=True)\n",
        "\n",
        "# Ensure the data is sorted by date\n",
        "df = df.sort_values('Interval_start')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(len(df) * 0.8)\n",
        "train, test = df[:train_size], df[train_size:]\n",
        "\n",
        "# Fit the ARIMA model\n",
        "model = ARIMA(train['Max_total'], order=(5,1,0))\n",
        "model_fit = model.fit()\n",
        "\n",
        "# Make predictions\n",
        "predictions = model_fit.forecast(steps=len(test))\n",
        "test['Predicted'] = predictions\n",
        "\n",
        "# Add status column\n",
        "test['Status'] = np.where(test['Predicted'] >= 1, 'Overflow', 'Normal')\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(test['Max_total'], test['Predicted'])\n",
        "rmse = np.sqrt(mse)\n",
        "accuracy = accuracy_score(test['Status'], np.where(test['Max_total'] >= 1, 'Overflow', 'Normal'))\n",
        "f1 = f1_score(test['Status'], np.where(test['Max_total'] >= 1, 'Overflow', 'Normal'), pos_label='Overflow')\n",
        "correlation = test[['Max_total', 'Predicted']].corr().iloc[0, 1]\n",
        "\n",
        "# Print metrics\n",
        "print(f'MSE: {mse}')\n",
        "print(f'RMSE: {rmse}')\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Correlation: {correlation}')\n",
        "\n",
        "# Save the results\n",
        "test.to_csv('Predicted_Intervals.csv', index=False)\n"
      ],
      "metadata": {
        "id": "mTZrMgulx30Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HV7or_XYbb7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Intervals.csv', parse_dates=['Interval_start'], dayfirst=True)\n",
        "\n",
        "# Ensure the data is sorted by date\n",
        "df = df.sort_values('Interval_start')\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "df['Max_total'] = scaler.fit_transform(df[['Max_total']])\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "def create_dataset(data, time_step=1):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data)-time_step-1):\n",
        "        a = data[i:(i+time_step), 0]\n",
        "        X.append(a)\n",
        "        Y.append(data[i + time_step, 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(len(df) * 0.8)\n",
        "train, test = df[:train_size], df[train_size:]\n",
        "\n",
        "# Reshape the data\n",
        "time_step = 30  # 30 minutes\n",
        "X_train, y_train = create_dataset(train[['Max_total']].values, time_step)\n",
        "X_test, y_test = create_dataset(test[['Max_total']].values, time_step)\n",
        "\n",
        "# Reshape input to be [samples, time steps, features] which is required for LSTM\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
        "model.add(LSTM(50, return_sequences=False))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1)\n",
        "\n",
        "# Make predictions for the next year\n",
        "future_steps = 365 * 24 * 2  # Assuming 30-second intervals for a year\n",
        "last_data = df['Max_total'].values[-time_step:]\n",
        "future_predictions = []\n",
        "\n",
        "for _ in range(future_steps):\n",
        "    last_data_reshaped = last_data.reshape(1, -1, 1)\n",
        "    next_pred = model.predict(last_data_reshaped)\n",
        "    future_predictions.append(next_pred[0, 0])\n",
        "    last_data = np.append(last_data[1:], next_pred)\n",
        "\n",
        "# Inverse transform to get actual values\n",
        "future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
        "\n",
        "# Create a DataFrame for future predictions\n",
        "future_dates = pd.date_range(start=df['Interval_start'].iloc[-1], periods=future_steps, freq='30S')\n",
        "future_df = pd.DataFrame({'Interval_start': future_dates, 'Predicted': future_predictions.flatten()})\n",
        "\n",
        "# Add status column\n",
        "future_df['Status'] = np.where(future_df['Predicted'] >= 1, 'Overflow', 'Normal')\n",
        "\n",
        "# Calculate metrics on the test set\n",
        "test_predict = model.predict(X_test)\n",
        "test_predict = scaler.inverse_transform(test_predict)\n",
        "y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "mse = mean_squared_error(y_test, test_predict)\n",
        "rmse = np.sqrt(mse)\n",
        "accuracy = accuracy_score(np.where(y_test >= 1, 'Overflow', 'Normal'), np.where(test_predict >= 1, 'Overflow', 'Normal'))\n",
        "f1 = f1_score(np.where(y_test >= 1, 'Overflow', 'Normal'), np.where(test_predict >= 1, 'Overflow', 'Normal'), pos_label='Overflow')\n",
        "correlation = np.corrcoef(y_test[:, 0], test_predict[:, 0])[0, 1]\n",
        "\n",
        "# Print metrics\n",
        "print(f'MSE: {mse}')\n",
        "print(f'RMSE: {rmse}')\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Correlation: {correlation}')\n",
        "\n",
        "# Save the results\n",
        "future_df.to_csv('Predicted_Intervals_2024.csv', index=False)\n",
        "\n",
        "# Plot the predictions\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(future_df['Interval_start'], future_df['Predicted'], label='Predicted')\n",
        "plt.axhline(y=1, color='r', linestyle='--', label='Overflow Threshold')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Max_total')\n",
        "plt.title('Forecasted Max_total for 2024')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Vsh9yO0v7jFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "848d791c-87c6-4192-91e9-da1ceb849988"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m7502/7502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 37ms/step - loss: 3.6226e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m7502/7502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 38ms/step - loss: 1.3912e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m7502/7502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 37ms/step - loss: 1.3041e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m7502/7502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 38ms/step - loss: 1.2376e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m7502/7502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 37ms/step - loss: 1.1753e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m7502/7502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 38ms/step - loss: 1.1404e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m7502/7502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 37ms/step - loss: 1.0563e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m4777/7502\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 37ms/step - loss: 9.5876e-05"
          ]
        }
      ]
    }
  ]
}