{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnrdzL+rpWccnBDIq7GQbS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Minakshi654/Modelname/blob/main/Text%20Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_UrrzYFvsEw"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('daily_dialog')"
      ],
      "metadata": {
        "id": "UuynxPVLvwYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "ib9v4rsovwa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining Text\n",
        "def conv_to_list_function(examples):\n",
        "    return [\"\".join(x) for x in examples]\n",
        "\n",
        "#cleaning first 1000 dialogs\n",
        "text_list = conv_to_list_function(dataset['train'][:1000]['dialog'])"
      ],
      "metadata": {
        "id": "rqO2d7Ilvwdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_list)"
      ],
      "metadata": {
        "id": "vgXaDdPBvwfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''\n",
        "for x in text_list:\n",
        "  text += x\n",
        "print('Length of the full text: ', len(text))\n",
        "print('Printing first 150 characters of the string: \\n', text[:150])"
      ],
      "metadata": {
        "id": "l6DVTrKRvwia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'w') as file:\n",
        "  file.write(text)"
      ],
      "metadata": {
        "id": "PeH1J1L3vwlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, TFAutoModelForCausalLM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "MArsCteFvwni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')"
      ],
      "metadata": {
        "id": "Z18kbyxcvwqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer([\" \".join(x) for x in examples[\"dialog\"]])"
      ],
      "metadata": {
        "id": "m5svXQlHvwse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    remove_columns = dataset[\"train\"].column_names,\n",
        ")"
      ],
      "metadata": {
        "id": "QqwA3LKtvwvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "Fr8i47i2yaPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128\n",
        "\n",
        "def group_texts(examples):\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    if total_length >= block_size:\n",
        "        total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of block_size.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "id": "j08h3sd0yaRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_dataset = tokenized_dataset.map(group_texts, batched=True, num_proc=4)\n",
        "lm_dataset"
      ],
      "metadata": {
        "id": "m0G22aMMyaVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"tf\")\n"
      ],
      "metadata": {
        "id": "Vmz0zF-xyaZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForCausalLM\n",
        "model = TFAutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
      ],
      "metadata": {
        "id": "CpQcoo5FyabU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    lm_dataset[\"train\"],\n",
        "    shuffle = True,\n",
        "    batch_size = 32,\n",
        "    collate_fn = data_collator,\n",
        ")\n",
        "\n",
        "tf_val_set = model.prepare_tf_dataset(\n",
        "    lm_dataset[\"validation\"],\n",
        "    shuffle = False,\n",
        "    batch_size = 32,\n",
        "    collate_fn = data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "xRZqRISfyadY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train_set"
      ],
      "metadata": {
        "id": "97kMnCwjyaf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_val_set"
      ],
      "metadata": {
        "id": "-DSv5SExyahv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForCausalLM, create_optimizer\n",
        "from tensorflow.keras.optimizers import Adam # Import Adam from tf.keras.optimizers\n",
        "\n",
        "# Create a Hugging Face compatible optimizer using create_optimizer\n",
        "num_train_steps = len(tf_train_set) # Replace with the actual number of training steps\n",
        "optimizer, lr_schedule = create_optimizer(\n",
        "    init_lr=0.001,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=num_train_steps\n",
        ")\n",
        "\n",
        "# Compile the model using the Hugging Face compatible optimizer\n",
        "model.compile(optimizer=optimizer)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "MPvGOYTezS--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x=tf_train_set, validation_data=tf_val_set, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBVKXvLvyanW",
        "outputId": "fcc4eb83-3e13-4b1e-8ce7-f24d20e7c194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            " 29/348 [=>............................] - ETA: 6:47:12 - loss: 4.1978"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "id": "TZqpeZ0Lyaq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YboLFJiuvwxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = 'I feel great today. It must be a warm and sunny day'\n",
        "input_ids = tokenizer.encode(input, return_tensors=\"tf\")\n",
        "input_ids"
      ],
      "metadata": {
        "id": "pAAWT5z1vw1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_ids = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2)\n",
        "output_ids"
      ],
      "metadata": {
        "id": "_6P9KY4Q0HzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "output"
      ],
      "metadata": {
        "id": "qm9OC_V0z8yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenized_dataset['test'][0]['input_ids']\n",
        "input_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
        "input_text"
      ],
      "metadata": {
        "id": "JBW0PQVrz8vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
        "input_ids"
      ],
      "metadata": {
        "id": "V1AWgqPSz8sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_ids =  model.generate(input_ids, max_length=input_ids.shape[1]+50, num_beams=5, no_repeat_ngram_size=2)\n",
        "output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "output"
      ],
      "metadata": {
        "id": "Te9yxtwwz8p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model(input_ids).logits\n",
        "probs = tf.nn.softmax(logits, axis=-1)\n",
        "target_ids = input_ids[:, 1:]\n",
        "loss = tf.keras.losses.sparse_categorical_crossentropy(target_ids, logits[:, :-1, :])\n",
        "perplexity = tf.exp(tf.reduce_mean(loss))\n",
        "\n",
        "print(\"Output:\", output)\n",
        "print(\"Perplexity:\", perplexity.numpy())"
      ],
      "metadata": {
        "id": "umxX31bCz8nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenized_dataset['test'][:10]['input_ids']\n",
        "\n",
        "perplexities = []\n",
        "outputs = []\n",
        "\n",
        "for input_id in input_ids:\n",
        "  input_text = tokenizer.decode(input_id, skip_special_tokens=True)\n",
        "  temp_input_id = tokenizer.encode(input_text, return_tensors='tf')\n",
        "  output_ids =  model.generate(temp_input_id, max_length=temp_input_id.shape[1]+50, num_beams=5, no_repeat_ngram_size=2)\n",
        "  output = tokenizer.decode(output_ids[0][-50:], skip_special_tokens=True)\n",
        "  outputs.append({\n",
        "      'input' : input_text,\n",
        "      'output' : output\n",
        "  })\n",
        "\n",
        "  #Evaluation\n",
        "  logits = model(temp_input_id).logits\n",
        "  probs = tf.nn.softmax(logits, axis=-1)\n",
        "  target_id = temp_input_id[:, 1:]\n",
        "  loss = tf.keras.losses.sparse_categorical_crossentropy(target_id, logits[:, :-1, :])\n",
        "  perplexity = tf.exp(tf.reduce_mean(loss))\n",
        "  perplexities.append(perplexity.numpy())\n",
        "  print('done')"
      ],
      "metadata": {
        "id": "DKgT-yFCz8lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# an output sample\n",
        "outputs[6]"
      ],
      "metadata": {
        "id": "wZuYu5uez8jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perplexities = np.array(perplexities)\n",
        "np.mean(perplexities)"
      ],
      "metadata": {
        "id": "YEentrfs0nhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZwV9FNpv0neL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yAoy4lhg0nbu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}