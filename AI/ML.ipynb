{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP4slDIbwp1p1BeGEWQE8tg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Minakshi654/Modelname/blob/main/AI/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JopUmC3itKnQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df_merged = pd.read_csv(\"/content/merged_file.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged [' total'] = df_merged ['[xBackground__]'] + df_merged ['[xInteractive__]']"
      ],
      "metadata": {
        "id": "Syl4lG6o93pV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "xK_XE0xA93l2",
        "outputId": "48923171-4e3d-4bcb-fdf0-f5908cd82ead"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Converted_Date  Month  Year  [xBackground__]  [xInteractive__]  \\\n",
              "0  2023-10-03 00:00:00     10  2023         0.114179               0.0   \n",
              "1  2023-10-03 00:00:30     10  2023         0.114179               0.0   \n",
              "2  2023-10-03 00:01:00     10  2023         0.114179               0.0   \n",
              "3  2023-10-03 00:01:30     10  2023         0.114179               0.0   \n",
              "4  2023-10-03 00:02:00     10  2023         0.114179               0.0   \n",
              "\n",
              "      total  \n",
              "0  0.114179  \n",
              "1  0.114179  \n",
              "2  0.114179  \n",
              "3  0.114179  \n",
              "4  0.114179  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f105d3a4-cc85-4ec4-9b76-e11f6402b58c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Converted_Date</th>\n",
              "      <th>Month</th>\n",
              "      <th>Year</th>\n",
              "      <th>[xBackground__]</th>\n",
              "      <th>[xInteractive__]</th>\n",
              "      <th>total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-10-03 00:00:00</td>\n",
              "      <td>10</td>\n",
              "      <td>2023</td>\n",
              "      <td>0.114179</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.114179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-10-03 00:00:30</td>\n",
              "      <td>10</td>\n",
              "      <td>2023</td>\n",
              "      <td>0.114179</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.114179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-10-03 00:01:00</td>\n",
              "      <td>10</td>\n",
              "      <td>2023</td>\n",
              "      <td>0.114179</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.114179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-10-03 00:01:30</td>\n",
              "      <td>10</td>\n",
              "      <td>2023</td>\n",
              "      <td>0.114179</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.114179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-10-03 00:02:00</td>\n",
              "      <td>10</td>\n",
              "      <td>2023</td>\n",
              "      <td>0.114179</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.114179</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f105d3a4-cc85-4ec4-9b76-e11f6402b58c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f105d3a4-cc85-4ec4-9b76-e11f6402b58c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f105d3a4-cc85-4ec4-9b76-e11f6402b58c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7850eda5-6c5e-4127-90f8-ecbf06987608\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7850eda5-6c5e-4127-90f8-ecbf06987608')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7850eda5-6c5e-4127-90f8-ecbf06987608 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_merged"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0bvfJ9MACzL",
        "outputId": "0e57f46d-546a-4195-c284-df6298791086"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1753685, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Convert datetime format\n",
        "df_merged['Converted_Date'] = pd.to_datetime(df_merged['Converted_Date'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Group data by 2-minute intervals\n",
        "grouped = df_merged.groupby(pd.Grouper(key='Converted_Date', freq='30Min'))[' total'].max()\n",
        "\n",
        "# Create result dataframe\n",
        "result_df = pd.DataFrame({\n",
        "    'Year': grouped.index.year,\n",
        "    'Month': grouped.index.month,\n",
        "    'Day': grouped.index.day,\n",
        "    'Time': grouped.index.time,\n",
        "    'Interval Start': grouped.index,\n",
        "    'Max Total': grouped.values\n",
        "})\n",
        "\n",
        "# Define status column\n",
        "result_df['status'] = result_df['Max Total'].apply(lambda x: 'Overflow' if x > 1 else 'Normal')\n",
        "\n",
        "# Define application categories\n",
        "app_names = [\"Pramaco\", \"Xref\", \"BBMSP\", \"Ardsorep\", \"SARA\", \"ACROSS\", \"SALORMON\"]\n",
        "overflow_indices = result_df[result_df['Max Total'] >= 1].index\n",
        "\n",
        "# Assign applications randomly, ensuring \"Other Application\" is most frequent\n",
        "np.random.seed(42)\n",
        "random_apps = np.random.choice(app_names, size=len(overflow_indices), replace=True)\n",
        "\n",
        "# Create weighted list with \"Other Application\" appearing more frequently\n",
        "weighted_apps = np.append(random_apps, [\"Other Application\"] * int(len(overflow_indices) * 1.5))\n",
        "\n",
        "# Shuffle for randomness\n",
        "np.random.shuffle(weighted_apps)\n",
        "\n",
        "# Assign applications to overflow cases\n",
        "result_df['Application'] = \"Other Application\"  # Default\n",
        "for i, idx in enumerate(overflow_indices):\n",
        "    result_df.loc[idx, 'Application'] = weighted_apps[i]\n",
        "\n",
        "# Save updated CSV\n",
        "result_df.to_csv('grouped_30_intervals_updated_new.csv', index=False)\n",
        "\n",
        "# Count overflow occurrences by application\n",
        "app_overflow_counts = result_df[result_df['Max Total'] >= 1]['Application'].value_counts()\n",
        "\n",
        "# Calculate percentages\n",
        "app_overflow_percentages = (app_overflow_counts / len(overflow_indices)) * 100\n",
        "\n",
        "# Print results\n",
        "print(f\"Total number of overflows: {len(overflow_indices)}\")\n",
        "print(\"\\nPercentage of overflows by application:\\n\")\n",
        "print(app_overflow_percentages)\n",
        "\n",
        "# Display sample output\n",
        "print(result_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hY9rLxGGcOl",
        "outputId": "851a2c79-1d30-4a6f-b1d6-5c8d2aa10006"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of overflows: 286\n",
            "\n",
            "Percentage of overflows by application:\n",
            "\n",
            "Application\n",
            "Other Application    61.188811\n",
            "Ardsorep              8.391608\n",
            "Pramaco               5.944056\n",
            "ACROSS                5.594406\n",
            "SALORMON              5.244755\n",
            "SARA                  5.244755\n",
            "Xref                  4.195804\n",
            "BBMSP                 4.195804\n",
            "Name: count, dtype: float64\n",
            "   Year  Month  Day      Time      Interval Start  Max Total  status  \\\n",
            "0  2023     10    3  00:00:00 2023-10-03 00:00:00   0.157106  Normal   \n",
            "1  2023     10    3  00:30:00 2023-10-03 00:30:00   0.174429  Normal   \n",
            "2  2023     10    3  01:00:00 2023-10-03 01:00:00   0.117271  Normal   \n",
            "3  2023     10    3  01:30:00 2023-10-03 01:30:00   0.114340  Normal   \n",
            "4  2023     10    3  02:00:00 2023-10-03 02:00:00   0.140475  Normal   \n",
            "\n",
            "         Application  \n",
            "0  Other Application  \n",
            "1  Other Application  \n",
            "2  Other Application  \n",
            "3  Other Application  \n",
            "4  Other Application  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import concurrent.futures\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv(\"/content/grouped_30_intervals_updated_new.csv\")\n",
        "\n",
        "# Feature Engineering\n",
        "data['Interval Start'] = pd.to_datetime(data['Interval Start'])\n",
        "data['Month'] = data['Interval Start'].dt.month\n",
        "data['Day'] = data['Interval Start'].dt.day\n",
        "data['Hour'] = data['Interval Start'].dt.hour\n",
        "\n",
        "# Encoding categorical variables\n",
        "label_enc = LabelEncoder()\n",
        "data['Application'] = label_enc.fit_transform(data['Application'])\n",
        "data['status'] = label_enc.fit_transform(data['status'])\n",
        "\n",
        "# Selecting features and target\n",
        "X = data[['Month', 'Day', 'Hour', 'Application']]\n",
        "y = data['Max Total']\n",
        "y = y.fillna(y.mean())\n",
        "# Splitting dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
        "    \"XGBoost\": XGBRegressor(),\n",
        "    \"LightGBM\": LGBMRegressor(),\n",
        "    \"Support Vector Regression\": SVR(kernel=\"rbf\")\n",
        "}\n",
        "\n",
        "# Function to train and evaluate models\n",
        "def train_model(model_name, model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    return model_name, rmse, r2\n",
        "\n",
        "# Parallel Execution\n",
        "results = {}\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    futures = {executor.submit(train_model, name, model, X_train, y_train, X_test, y_test): name for name, model in models.items()}\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "        model_name, rmse, r2 = future.result()\n",
        "        results[model_name] = {\"RMSE\": rmse, \"R² Score\": r2}\n",
        "\n",
        "# Display results\n",
        "results_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
        "print(\"\\nModel Performance Comparison:\")\n",
        "print(results_df)\n",
        "\n",
        "# Plot results\n",
        "sns.barplot(x=results_df.index, y=results_df[\"R² Score\"])\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Model Comparison - R² Score\")\n",
        "plt.ylabel(\"R² Score\")\n",
        "plt.show()\n",
        "\n",
        "# Predict future overflow frequency\n",
        "future_data = pd.DataFrame({\n",
        "    \"Month\": [6, 7, 8],  # Example upcoming months\n",
        "    \"Day\": [15, 16, 17],\n",
        "    \"Hour\": [10, 12, 14],\n",
        "    \"Application\": [0, 1, 2]  # Encoded applications\n",
        "})\n",
        "\n",
        "future_data = scaler.transform(future_data)\n",
        "best_model = models[max(results, key=lambda x: results[x][\"R² Score\"])]\n",
        "future_predictions = best_model.predict(future_data)\n",
        "\n",
        "print(\"\\nPredicted Max Total for upcoming months:\")\n",
        "print(future_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 908
        },
        "id": "4ZA4mMj8GyFg",
        "outputId": "5535dd0f-c5d2-4d66-ad71-057a35e8709b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance Comparison:\n",
            "                               RMSE  R² Score\n",
            "Linear Regression          0.178778 -0.000481\n",
            "LightGBM                   0.134156  0.436623\n",
            "XGBoost                    0.133900  0.438767\n",
            "Gradient Boosting          0.147587  0.318165\n",
            "Random Forest              0.156810  0.230289\n",
            "Support Vector Regression  0.150920  0.287022\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted Max Total for upcoming months:\n",
            "[0.83569527 1.2123308  0.8563946 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "future_data = scaler.transform(future_data)\n",
        "best_model = models[max(results, key=lambda x: results[x][\"R² Score\"])]\n",
        "future_predictions = best_model.predict(future_data)\n",
        "\n",
        "print(\"\\nPredicted Max Total for upcoming months:\")\n",
        "print(future_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksr6m0Z2Fz3C",
        "outputId": "87a22fa5-62ea-4110-bcd6-d49006d4c2ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted Max Total for upcoming months:\n",
            "[0.86724174 0.86724174 0.86724174]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import concurrent.futures\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"/content/grouped_30_intervals_updated_new.csv\")\n",
        "\n",
        "# Convert timestamps\n",
        "data['Interval Start'] = pd.to_datetime(data['Interval Start'])\n",
        "data['Month'] = data['Interval Start'].dt.month\n",
        "data['Day'] = data['Interval Start'].dt.day\n",
        "data['Hour'] = data['Interval Start'].dt.hour\n",
        "data['Minute'] = data['Interval Start'].dt.minute // 30  # Group into 30-min intervals\n",
        "\n",
        "# Encode categorical variables\n",
        "label_enc = LabelEncoder()\n",
        "data['Application'] = label_enc.fit_transform(data['Application'])\n",
        "\n",
        "# Select features and target\n",
        "X = data[['Month', 'Day', 'Hour', 'Minute', 'Application']]\n",
        "y = data['Max Total']\n",
        "y = y.fillna(y.mean())\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
        "    \"XGBoost\": XGBRegressor(),\n",
        "    \"LightGBM\": LGBMRegressor()\n",
        "}\n",
        "\n",
        "# Parallel model training\n",
        "def train_model(model_name, model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    return model_name, predictions\n",
        "\n",
        "results = {}\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    futures = {executor.submit(train_model, name, model, X_train, y_train, X_test, y_test): name for name, model in models.items()}\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "        model_name, predictions = future.result()\n",
        "        results[model_name] = predictions\n",
        "\n",
        "# Select best model (highest R² score)\n",
        "best_model_name = max(results, key=lambda x: models[x].score(X_test, y_test))\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "# Predict future values (June 2025 - December 2025)\n",
        "future_dates = pd.date_range(start=\"2025-06-01\", end=\"2025-12-31\", freq=\"30min\")\n",
        "future_data = pd.DataFrame({\n",
        "    \"Month\": future_dates.month,\n",
        "    \"Day\": future_dates.day,\n",
        "    \"Hour\": future_dates.hour,\n",
        "    \"Minute\": future_dates.minute // 30,\n",
        "    \"Application\": np.random.choice(data['Application'].unique(), size=len(future_dates))\n",
        "})\n",
        "\n",
        "future_data_scaled = scaler.transform(future_data)\n",
        "future_predictions = best_model.predict(future_data_scaled)\n",
        "\n",
        "# Save predictions to CSV\n",
        "future_data[\"Predicted Max Total\"] = future_predictions\n",
        "future_data.to_csv(\"future_predictions.csv\", index=False)\n",
        "\n",
        "# Count overflows (Max Total >= 1)\n",
        "num_overflows = (future_data[\"Predicted Max Total\"] >= 1).sum()\n",
        "print(f\"\\nTotal number of predicted overflows: {num_overflows}\")\n",
        "\n",
        "# Overflow percentage per application\n",
        "overflow_counts = future_data[future_data[\"Predicted Max Total\"] >= 1].groupby(\"Application\").size()\n",
        "overflow_percentage = (overflow_counts / num_overflows) * 100\n",
        "overflow_percentage_df = pd.DataFrame({\"Application\": overflow_percentage.index, \"Overflow Percentage\": overflow_percentage.values})\n",
        "\n",
        "# Save overflow percentages\n",
        "overflow_percentage_df.to_csv(\"application_overflows.csv\", index=False)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nPredicted overflow occurrences by application:\")\n",
        "print(overflow_percentage_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgsO3JD4n73n",
        "outputId": "b91c7fa9-2c88-4dd7-e539-0069d86fcc0a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total number of predicted overflows: 3874\n",
            "\n",
            "Predicted overflow occurrences by application:\n",
            "   Application  Overflow Percentage\n",
            "0            0            14.377904\n",
            "1            1            14.894166\n",
            "2            2            13.603511\n",
            "3            4            13.577697\n",
            "4            5            14.558596\n",
            "5            6            12.958183\n",
            "6            7            16.029943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import concurrent.futures\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"/content/grouped_30_intervals_updated_new.csv\")\n",
        "\n",
        "# Convert timestamps\n",
        "data['Interval Start'] = pd.to_datetime(data['Interval Start'])\n",
        "data['Month'] = data['Interval Start'].dt.month\n",
        "data['Day'] = data['Interval Start'].dt.day\n",
        "data['Hour'] = data['Interval Start'].dt.hour\n",
        "data['Minute'] = data['Interval Start'].dt.minute // 30  # Group into 30-min intervals\n",
        "\n",
        "# Encode categorical variables\n",
        "label_enc = LabelEncoder()\n",
        "data['Application'] = label_enc.fit_transform(data['Application'])\n",
        "\n",
        "# Selecting features and target\n",
        "X = data[['Month', 'Day', 'Hour', 'Minute', 'Application']]\n",
        "y = data['Max Total']\n",
        "y = y.fillna(y.mean())\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define models with optimized hyperparameters\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestRegressor(n_estimators=200, max_depth=10),\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=150, learning_rate=0.1),\n",
        "    \"XGBoost\": XGBRegressor(n_estimators=150, learning_rate=0.1),\n",
        "    \"LightGBM\": LGBMRegressor(n_estimators=150, learning_rate=0.1)\n",
        "}\n",
        "\n",
        "# Parallel model training and accuracy evaluation\n",
        "def train_and_evaluate_model(model_name, model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    return model_name, rmse, r2, mae\n",
        "\n",
        "results = {}\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    futures = {executor.submit(train_and_evaluate_model, name, model, X_train, y_train, X_test, y_test): name for name, model in models.items()}\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "        model_name, rmse, r2, mae = future.result()\n",
        "        results[model_name] = {\"RMSE\": rmse, \"R² Score\": r2, \"MAE\": mae}\n",
        "\n",
        "# Display model performance\n",
        "results_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
        "print(\"\\nUpdated Model Performance Comparison:\")\n",
        "print(results_df)\n",
        "\n",
        "# Select best-performing model\n",
        "best_model_name = results_df[\"R² Score\"].idxmax()\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "# Predict future values (June 2025 - December 2025)\n",
        "future_dates = pd.date_range(start=\"2025-06-01\", end=\"2025-12-31\", freq=\"30min\")\n",
        "future_data = pd.DataFrame({\n",
        "    \"Month\": future_dates.month,\n",
        "    \"Day\": future_dates.day,\n",
        "    \"Hour\": future_dates.hour,\n",
        "    \"Minute\": future_dates.minute // 30,\n",
        "    \"Application\": np.random.choice(data['Application'].unique(), size=len(future_dates))\n",
        "})\n",
        "\n",
        "future_data_scaled = scaler.transform(future_data)\n",
        "future_predictions = best_model.predict(future_data_scaled)\n",
        "\n",
        "# Save predictions to CSV\n",
        "future_data[\"Predicted Max Total\"] = future_predictions\n",
        "future_data.to_csv(\"future_predictions_new.csv\", index=False)\n",
        "\n",
        "# Count overflows (Max Total >= 1)\n",
        "num_overflows = (future_data[\"Predicted Max Total\"] >= 1).sum()\n",
        "print(f\"\\nTotal number of predicted overflows: {num_overflows}\")\n",
        "\n",
        "# Overflow percentage per application\n",
        "overflow_counts = future_data[future_data[\"Predicted Max Total\"] >= 1].groupby(\"Application\").size()\n",
        "overflow_percentage = (overflow_counts / num_overflows) * 100\n",
        "overflow_percentage_df = pd.DataFrame({\"Application\": overflow_percentage.index, \"Overflow Percentage\": overflow_percentage.values})\n",
        "\n",
        "# Save overflow percentages\n",
        "overflow_percentage_df.to_csv(\"application_overflows.csv\", index=False)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nUpdated predicted overflow occurrences by application:\")\n",
        "print(overflow_percentage_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGmtc_14n70L",
        "outputId": "2b1c702d-b5f2-4184-9fbe-696bd39494fc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Updated Model Performance Comparison:\n",
            "                       RMSE  R² Score       MAE\n",
            "XGBoost            0.135975  0.421242  0.094981\n",
            "LightGBM           0.132245  0.452559  0.091800\n",
            "Gradient Boosting  0.146305  0.329959  0.104360\n",
            "Random Forest      0.146724  0.326121  0.102373\n",
            "\n",
            "Total number of predicted overflows: 4253\n",
            "\n",
            "Updated predicted overflow occurrences by application:\n",
            "   Application  Overflow Percentage\n",
            "0            0            13.707971\n",
            "1            1            13.778509\n",
            "2            2            13.472843\n",
            "3            4            14.201740\n",
            "4            5            14.342817\n",
            "5            6            14.813073\n",
            "6            7            15.683047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count overflows per application per month\n",
        "overflow_counts_monthly = future_data[future_data[\"Predicted Max Total\"] >= 1].groupby([\"Month\", \"Application\"]).size().reset_index(name=\"Overflow Count\")\n",
        "\n",
        "# Save to CSV\n",
        "overflow_counts_monthly.to_csv(\"monthly_application_overflows.csv\", index=False)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nPredicted Overflows Per Application Per Month:\")\n",
        "print(overflow_counts_monthly)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqHIPeAZn7xi",
        "outputId": "b1f52e8d-4a19-4875-b48e-c659c6bff9ba"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted Overflows Per Application Per Month:\n",
            "    Month  Application  Overflow Count\n",
            "0       6            0              82\n",
            "1       6            1              74\n",
            "2       6            2              85\n",
            "3       6            4              95\n",
            "4       6            5              95\n",
            "5       6            6              79\n",
            "6       6            7              95\n",
            "7       7            0              81\n",
            "8       7            1              88\n",
            "9       7            2              76\n",
            "10      7            4              79\n",
            "11      7            5              88\n",
            "12      7            6             102\n",
            "13      7            7              86\n",
            "14      8            0              97\n",
            "15      8            1             101\n",
            "16      8            2              79\n",
            "17      8            4              70\n",
            "18      8            5              90\n",
            "19      8            6              98\n",
            "20      8            7              91\n",
            "21      9            0              85\n",
            "22      9            1              78\n",
            "23      9            2              80\n",
            "24      9            4              89\n",
            "25      9            5              81\n",
            "26      9            6              79\n",
            "27      9            7             110\n",
            "28     10            0              73\n",
            "29     10            1             104\n",
            "30     10            2              90\n",
            "31     10            4              84\n",
            "32     10            5              87\n",
            "33     10            6              82\n",
            "34     10            7             108\n",
            "35     11            0              79\n",
            "36     11            1              59\n",
            "37     11            2              81\n",
            "38     11            4             103\n",
            "39     11            5              85\n",
            "40     11            6              94\n",
            "41     11            7              93\n",
            "42     12            0              86\n",
            "43     12            1              82\n",
            "44     12            2              82\n",
            "45     12            4              84\n",
            "46     12            5              84\n",
            "47     12            6              96\n",
            "48     12            7              84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse label encoding\n",
        "application_mapping = dict(zip(label_enc.transform(label_enc.classes_), label_enc.classes_))\n",
        "future_data[\"Application\"] = future_data[\"Application\"].map(application_mapping)\n",
        "\n",
        "# Count overflows per application per month with actual names\n",
        "overflow_counts_monthly = future_data[future_data[\"Predicted Max Total\"] >= 1].groupby([\"Month\", \"Application\"]).size().reset_index(name=\"Overflow Count\")\n",
        "\n",
        "# Save to CSV with real application names\n",
        "overflow_counts_monthly.to_csv(\"monthly_application_overflows.csv\", index=False)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nUpdated Predicted Overflows Per Application Per Month (With Names):\")\n",
        "print(overflow_counts_monthly)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvMWAkCXn7vH",
        "outputId": "8a63ef90-edef-4e30-877f-338bdd412337"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Updated Predicted Overflows Per Application Per Month (With Names):\n",
            "    Month Application  Overflow Count\n",
            "0       6      ACROSS              82\n",
            "1       6    Ardsorep              74\n",
            "2       6       BBMSP              85\n",
            "3       6     Pramaco              95\n",
            "4       6    SALORMON              95\n",
            "5       6        SARA              79\n",
            "6       6        Xref              95\n",
            "7       7      ACROSS              81\n",
            "8       7    Ardsorep              88\n",
            "9       7       BBMSP              76\n",
            "10      7     Pramaco              79\n",
            "11      7    SALORMON              88\n",
            "12      7        SARA             102\n",
            "13      7        Xref              86\n",
            "14      8      ACROSS              97\n",
            "15      8    Ardsorep             101\n",
            "16      8       BBMSP              79\n",
            "17      8     Pramaco              70\n",
            "18      8    SALORMON              90\n",
            "19      8        SARA              98\n",
            "20      8        Xref              91\n",
            "21      9      ACROSS              85\n",
            "22      9    Ardsorep              78\n",
            "23      9       BBMSP              80\n",
            "24      9     Pramaco              89\n",
            "25      9    SALORMON              81\n",
            "26      9        SARA              79\n",
            "27      9        Xref             110\n",
            "28     10      ACROSS              73\n",
            "29     10    Ardsorep             104\n",
            "30     10       BBMSP              90\n",
            "31     10     Pramaco              84\n",
            "32     10    SALORMON              87\n",
            "33     10        SARA              82\n",
            "34     10        Xref             108\n",
            "35     11      ACROSS              79\n",
            "36     11    Ardsorep              59\n",
            "37     11       BBMSP              81\n",
            "38     11     Pramaco             103\n",
            "39     11    SALORMON              85\n",
            "40     11        SARA              94\n",
            "41     11        Xref              93\n",
            "42     12      ACROSS              86\n",
            "43     12    Ardsorep              82\n",
            "44     12       BBMSP              82\n",
            "45     12     Pramaco              84\n",
            "46     12    SALORMON              84\n",
            "47     12        SARA              96\n",
            "48     12        Xref              84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm\n"
      ],
      "metadata": {
        "id": "sGrEcOe-4ozq",
        "outputId": "8294841d-d107-4e8b-bfd1-4604c02d05f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vllm in /usr/local/lib/python3.11/dist-packages (0.8.5.post1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vllm) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\n",
            "Requirement already satisfied: blake3 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.0.4)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.51.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.51.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->vllm) (0.30.2)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (4.25.7)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.115.12)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.15)\n",
            "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.76.2)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.4)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.2.1)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (7.1.0)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\n",
            "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.10.11)\n",
            "Requirement already satisfied: llguidance<0.8.0,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.7.19)\n",
            "Requirement already satisfied: outlines==0.1.11 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.11)\n",
            "Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.2.2)\n",
            "Requirement already satisfied: xgrammar==0.1.18 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.18)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.13.2)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.18.0)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.1.1.post5)\n",
            "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (26.4.0)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from vllm) (0.19.0)\n",
            "Requirement already satisfied: gguf>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.16.3)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm) (8.0.0)\n",
            "Requirement already satisfied: mistral_common>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.4->vllm) (1.5.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\n",
            "Requirement already satisfied: compressed-tensors==0.9.3 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.3)\n",
            "Requirement already satisfied: depyf==0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.18.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\n",
            "Requirement already satisfied: watchfiles in /usr/local/lib/python3.11/dist-packages (from vllm) (1.0.5)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (3.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm) (1.11.1.4)\n",
            "Requirement already satisfied: opentelemetry-sdk<1.27.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-api<1.27.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp<1.27.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.4.7)\n",
            "Requirement already satisfied: numba==0.61.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.61.2)\n",
            "Requirement already satisfied: ray!=2.44.*,>=2.43.0 in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (2.46.0)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.0+cu124)\n",
            "Requirement already satisfied: xformers==0.0.29.post2 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.0.29.post2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.8.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.4.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.61.2->vllm) (0.44.0)\n",
            "Requirement already satisfied: interegular in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (5.6.3)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (24.6.1)\n",
            "Requirement already satisfied: airportsdata in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (20250224)\n",
            "Requirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.1.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->vllm) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.46.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.7)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.34.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.0->huggingface-hub[hf_xet]>=0.30.0->vllm) (24.2)\n",
            "Requirement already satisfied: hf-xet>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.2.18)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->vllm) (3.21.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.26.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.71.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.26.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<1.27.0,>=1.26.0->vllm) (0.47b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.8)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.4.26)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.1->vllm) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.20.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.17.2)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.3)\n",
            "Requirement already satisfied: rich-toolkit>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.14.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2025.4.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.24.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.3)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5fGcWBqG5z2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "6zcnz51x5Ghg",
        "outputId": "4963736b-5fbb-4b00-f7fa-73b350023d18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May  8 12:36:36 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EHKINZ-m5GeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vqk_-u4L5Gbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM\n",
        "\n",
        "# Load the Qwen model with float16\n",
        "llm = LLM(model=\"Qwen/Qwen3-235B-A22B\", dtype=\"half\")\n",
        "\n",
        "# Ask a question\n",
        "response = llm.generate(\"What is the capital of France?\")\n",
        "print(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "HmY7a46o4owX",
        "outputId": "c9e33be9-8965-42a9-a2a8-f568919faa68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 05-08 12:44:31 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 05-08 12:44:31 [config.py:717] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.\n",
            "WARNING 05-08 12:44:31 [arg_utils.py:1525] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.\n",
            "INFO 05-08 12:44:31 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "INFO 05-08 12:44:31 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='Qwen/Qwen3-235B-A22B', speculative_config=None, tokenizer='Qwen/Qwen3-235B-A22B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen3-235B-A22B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "INFO 05-08 12:44:32 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 05-08 12:44:33 [cuda.py:289] Using XFormers backend.\n",
            "INFO 05-08 12:44:33 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 05-08 12:44:33 [model_runner.py:1108] Starting to load model Qwen/Qwen3-235B-A22B...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 13793 has 13.69 GiB memory in use. Of the allocated memory 13.57 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a079eb441798>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the Qwen model with float16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Qwen/Qwen3-235B-A22B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"half\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Ask a question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     )\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Create the Engine (autoselects V0 vs V1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    248\u001b[0m             engine_args=engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_engine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mengine_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV1LLMEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         return engine_cls.from_vllm_config(\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0musage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musage_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_vllm_config\u001b[0;34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mdisable_log_stats\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     ) -> \"LLMEngine\":\n\u001b[0;32m--> 486\u001b[0;31m         return cls(\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_executor_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                                                     mm_registry)\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"pooling\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sleeping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleeping_tags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\u001b[0m in \u001b[0;36m_init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollective_rpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_worker\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollective_rpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_device\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollective_rpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"load_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     def collective_rpc(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\u001b[0m in \u001b[0;36mcollective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2454\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2456\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     def save_sharded_state(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mDeviceMemoryProfiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mtime_before_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m                 assert supports_lora(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/__init__.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(vllm_config)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVllmConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_default_torch_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtarget_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_initialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0mweights_to_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36m_initialize_model\u001b[0;34m(vllm_config, prefix, model_class)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# new-style model class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_current_vllm_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_compile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     msg = (\"vLLM model class should accept `vllm_config` and `prefix` as \"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         self.model = Qwen3MoeModel(vllm_config=vllm_config,\n\u001b[0m\u001b[1;32m    489\u001b[0m                                    prefix=maybe_prefix(prefix, \"model\"))\n\u001b[1;32m    490\u001b[0m         self.lm_head = ParallelLMHead(config.vocab_size,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/compilation/decorators.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVllmConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mold_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvllm_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# for CompilationLevel.DYNAMO_AS_IS , the upper level model runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             prefix=f\"{prefix}.embed_tokens\")\n\u001b[0;32m--> 334\u001b[0;31m         self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             lambda prefix: Qwen3MoeDecoderLayer(config=config,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\u001b[0m in \u001b[0;36mmake_layers\u001b[0;34m(num_hidden_layers, layer_fn, prefix)\u001b[0m\n\u001b[1;32m    607\u001b[0m                                             get_pp_group().world_size)\n\u001b[1;32m    608\u001b[0m     modules = torch.nn.ModuleList(\n\u001b[0;32m--> 609\u001b[0;31m         [PPMissingLayer() for _ in range(start_layer)] + [\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mmaybe_offload_to_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{prefix}.{idx}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    608\u001b[0m     modules = torch.nn.ModuleList(\n\u001b[1;32m    609\u001b[0m         [PPMissingLayer() for _ in range(start_layer)] + [\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0mmaybe_offload_to_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{prefix}.{idx}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         ] + [PPMissingLayer() for _ in range(end_layer, num_hidden_layers)])\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m    334\u001b[0m         self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[1;32m    335\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             lambda prefix: Qwen3MoeDecoderLayer(config=config,\n\u001b[0m\u001b[1;32m    337\u001b[0m                                                 \u001b[0mcache_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                                                 \u001b[0mquant_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, cache_config, quant_config, prefix)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_experts\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             (layer_idx + 1) % config.decoder_sparse_step == 0):\n\u001b[0;32m--> 278\u001b[0;31m             self.mlp = Qwen3MoeSparseMoeBlock(config=config,\n\u001b[0m\u001b[1;32m    279\u001b[0m                                               \u001b[0mquant_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                                               prefix=f\"{prefix}.mlp\")\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, quant_config, prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 f\"the number of experts {config.num_experts}.\")\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         self.experts = FusedMoE(num_experts=config.num_experts,\n\u001b[0m\u001b[1;32m    114\u001b[0m                                 \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_experts_per_tok\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                                 \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_experts, top_k, hidden_size, intermediate_size, params_dtype, reduce_results, renormalize, use_grouped_topk, num_expert_group, topk_group, quant_config, tp_size, ep_size, dp_size, prefix, custom_routing_function, scoring_func, e_score_correction_bias, apply_router_weight_on_input, activation)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mmoe_quant_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"intermediate_size_full\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_method\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmoe_quant_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     def _load_per_tensor_weight_scale(self, shard_id: str,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\u001b[0m in \u001b[0;36mcreate_weights\u001b[0;34m(self, layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype, **extra_weight_attrs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# down_proj (row parallel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         w2_weight = torch.nn.Parameter(torch.empty(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mnum_experts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 13793 has 13.69 GiB memory in use. Of the allocated memory 13.57 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
      ],
      "metadata": {
        "id": "fSyAgiM27D4y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(model=\"Qwen/Qwen3-235B-A22B\", dtype=\"half\", max_num_batched_tokens=512)\n"
      ],
      "metadata": {
        "id": "6geiK1Jv7a2P",
        "outputId": "e12613e3-6a43-4424-fca7-7f7654cfcb25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 05-08 12:45:59 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 05-08 12:45:59 [config.py:717] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.\n",
            "WARNING 05-08 12:45:59 [arg_utils.py:1525] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.\n",
            "INFO 05-08 12:45:59 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
            "INFO 05-08 12:45:59 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='Qwen/Qwen3-235B-A22B', speculative_config=None, tokenizer='Qwen/Qwen3-235B-A22B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen3-235B-A22B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "INFO 05-08 12:46:00 [model_runner.py:1108] Starting to load model Qwen/Qwen3-235B-A22B...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.39 GiB is free. Process 13793 has 13.34 GiB memory in use. Of the allocated memory 13.23 GiB is allocated by PyTorch, and 991.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-85e3e59d4a28>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Qwen/Qwen3-235B-A22B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"half\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_batched_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     )\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Create the Engine (autoselects V0 vs V1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    248\u001b[0m             engine_args=engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_engine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mengine_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV1LLMEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         return engine_cls.from_vllm_config(\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0musage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musage_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_vllm_config\u001b[0;34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mdisable_log_stats\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     ) -> \"LLMEngine\":\n\u001b[0;32m--> 486\u001b[0;31m         return cls(\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_executor_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                                                     mm_registry)\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"pooling\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sleeping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleeping_tags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\u001b[0m in \u001b[0;36m_init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollective_rpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_worker\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollective_rpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_device\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollective_rpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"load_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     def collective_rpc(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\u001b[0m in \u001b[0;36mcollective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2454\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2456\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     def save_sharded_state(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mDeviceMemoryProfiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mtime_before_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m                 assert supports_lora(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/__init__.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(vllm_config)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVllmConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_default_torch_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtarget_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_initialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0mweights_to_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36m_initialize_model\u001b[0;34m(vllm_config, prefix, model_class)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# new-style model class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_current_vllm_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_compile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     msg = (\"vLLM model class should accept `vllm_config` and `prefix` as \"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         self.model = Qwen3MoeModel(vllm_config=vllm_config,\n\u001b[0m\u001b[1;32m    489\u001b[0m                                    prefix=maybe_prefix(prefix, \"model\"))\n\u001b[1;32m    490\u001b[0m         self.lm_head = ParallelLMHead(config.vocab_size,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/compilation/decorators.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVllmConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mold_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvllm_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# for CompilationLevel.DYNAMO_AS_IS , the upper level model runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             prefix=f\"{prefix}.embed_tokens\")\n\u001b[0;32m--> 334\u001b[0;31m         self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             lambda prefix: Qwen3MoeDecoderLayer(config=config,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\u001b[0m in \u001b[0;36mmake_layers\u001b[0;34m(num_hidden_layers, layer_fn, prefix)\u001b[0m\n\u001b[1;32m    607\u001b[0m                                             get_pp_group().world_size)\n\u001b[1;32m    608\u001b[0m     modules = torch.nn.ModuleList(\n\u001b[0;32m--> 609\u001b[0;31m         [PPMissingLayer() for _ in range(start_layer)] + [\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mmaybe_offload_to_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{prefix}.{idx}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    608\u001b[0m     modules = torch.nn.ModuleList(\n\u001b[1;32m    609\u001b[0m         [PPMissingLayer() for _ in range(start_layer)] + [\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0mmaybe_offload_to_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{prefix}.{idx}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         ] + [PPMissingLayer() for _ in range(end_layer, num_hidden_layers)])\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m    334\u001b[0m         self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[1;32m    335\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             lambda prefix: Qwen3MoeDecoderLayer(config=config,\n\u001b[0m\u001b[1;32m    337\u001b[0m                                                 \u001b[0mcache_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                                                 \u001b[0mquant_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, cache_config, quant_config, prefix)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_experts\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             (layer_idx + 1) % config.decoder_sparse_step == 0):\n\u001b[0;32m--> 278\u001b[0;31m             self.mlp = Qwen3MoeSparseMoeBlock(config=config,\n\u001b[0m\u001b[1;32m    279\u001b[0m                                               \u001b[0mquant_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                                               prefix=f\"{prefix}.mlp\")\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, quant_config, prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 f\"the number of experts {config.num_experts}.\")\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         self.experts = FusedMoE(num_experts=config.num_experts,\n\u001b[0m\u001b[1;32m    114\u001b[0m                                 \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_experts_per_tok\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                                 \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_experts, top_k, hidden_size, intermediate_size, params_dtype, reduce_results, renormalize, use_grouped_topk, num_expert_group, topk_group, quant_config, tp_size, ep_size, dp_size, prefix, custom_routing_function, scoring_func, e_score_correction_bias, apply_router_weight_on_input, activation)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mmoe_quant_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"intermediate_size_full\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_method\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmoe_quant_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     def _load_per_tensor_weight_scale(self, shard_id: str,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\u001b[0m in \u001b[0;36mcreate_weights\u001b[0;34m(self, layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype, **extra_weight_attrs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# down_proj (row parallel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         w2_weight = torch.nn.Parameter(torch.empty(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mnum_experts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.39 GiB is free. Process 13793 has 13.34 GiB memory in use. Of the allocated memory 13.23 GiB is allocated by PyTorch, and 991.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "lEbywNGb7g6b"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM\n",
        "import torch\n",
        "torch.set_default_device(\"cpu\")\n",
        "llm = LLM(model=\"Qwen/Qwen3-235B-A22B\", dtype=\"half\")\n"
      ],
      "metadata": {
        "id": "Mv8iSHD57oUM",
        "outputId": "2beb30b3-7e19-498e-8e0b-560c5bb6afab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 05-08 12:48:35 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 05-08 12:48:42 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 05-08 12:48:56 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\n",
            "WARNING 05-08 12:48:56 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
            "WARNING 05-08 12:48:56 [arg_utils.py:1525] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.\n",
            "INFO 05-08 12:48:56 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "INFO 05-08 12:48:56 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='Qwen/Qwen3-235B-A22B', speculative_config=None, tokenizer='Qwen/Qwen3-235B-A22B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen3-235B-A22B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "INFO 05-08 12:48:58 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 05-08 12:48:58 [cuda.py:289] Using XFormers backend.\n",
            "INFO 05-08 12:48:59 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 05-08 12:48:59 [model_runner.py:1108] Starting to load model Qwen/Qwen3-235B-A22B...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 35483 has 13.69 GiB memory in use. Of the allocated memory 13.57 GiB is allocated by PyTorch, and 8.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-60708825f3ef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Qwen/Qwen3-235B-A22B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"half\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     )\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Create the Engine (autoselects V0 vs V1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    248\u001b[0m             engine_args=engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_engine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mengine_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV1LLMEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         return engine_cls.from_vllm_config(\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0musage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musage_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_vllm_config\u001b[0;34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mdisable_log_stats\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     ) -> \"LLMEngine\":\n\u001b[0;32m--> 486\u001b[0;31m         return cls(\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_executor_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                                                     mm_registry)\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"pooling\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sleeping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleeping_tags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\u001b[0m in \u001b[0;36m_init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollective_rpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_worker\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollective_rpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_device\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollective_rpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"load_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     def collective_rpc(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\u001b[0m in \u001b[0;36mcollective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2454\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2456\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     def save_sharded_state(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mDeviceMemoryProfiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mtime_before_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m                 assert supports_lora(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/__init__.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(vllm_config)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVllmConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_default_torch_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtarget_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_initialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0mweights_to_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36m_initialize_model\u001b[0;34m(vllm_config, prefix, model_class)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# new-style model class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_current_vllm_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_compile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     msg = (\"vLLM model class should accept `vllm_config` and `prefix` as \"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         self.model = Qwen3MoeModel(vllm_config=vllm_config,\n\u001b[0m\u001b[1;32m    489\u001b[0m                                    prefix=maybe_prefix(prefix, \"model\"))\n\u001b[1;32m    490\u001b[0m         self.lm_head = ParallelLMHead(config.vocab_size,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/compilation/decorators.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVllmConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mold_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvllm_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# for CompilationLevel.DYNAMO_AS_IS , the upper level model runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             prefix=f\"{prefix}.embed_tokens\")\n\u001b[0;32m--> 334\u001b[0;31m         self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             lambda prefix: Qwen3MoeDecoderLayer(config=config,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\u001b[0m in \u001b[0;36mmake_layers\u001b[0;34m(num_hidden_layers, layer_fn, prefix)\u001b[0m\n\u001b[1;32m    607\u001b[0m                                             get_pp_group().world_size)\n\u001b[1;32m    608\u001b[0m     modules = torch.nn.ModuleList(\n\u001b[0;32m--> 609\u001b[0;31m         [PPMissingLayer() for _ in range(start_layer)] + [\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mmaybe_offload_to_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{prefix}.{idx}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    608\u001b[0m     modules = torch.nn.ModuleList(\n\u001b[1;32m    609\u001b[0m         [PPMissingLayer() for _ in range(start_layer)] + [\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0mmaybe_offload_to_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{prefix}.{idx}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         ] + [PPMissingLayer() for _ in range(end_layer, num_hidden_layers)])\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m    334\u001b[0m         self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[1;32m    335\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             lambda prefix: Qwen3MoeDecoderLayer(config=config,\n\u001b[0m\u001b[1;32m    337\u001b[0m                                                 \u001b[0mcache_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                                                 \u001b[0mquant_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, cache_config, quant_config, prefix)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_experts\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             (layer_idx + 1) % config.decoder_sparse_step == 0):\n\u001b[0;32m--> 278\u001b[0;31m             self.mlp = Qwen3MoeSparseMoeBlock(config=config,\n\u001b[0m\u001b[1;32m    279\u001b[0m                                               \u001b[0mquant_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                                               prefix=f\"{prefix}.mlp\")\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, quant_config, prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 f\"the number of experts {config.num_experts}.\")\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         self.experts = FusedMoE(num_experts=config.num_experts,\n\u001b[0m\u001b[1;32m    114\u001b[0m                                 \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_experts_per_tok\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                                 \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_experts, top_k, hidden_size, intermediate_size, params_dtype, reduce_results, renormalize, use_grouped_topk, num_expert_group, topk_group, quant_config, tp_size, ep_size, dp_size, prefix, custom_routing_function, scoring_func, e_score_correction_bias, apply_router_weight_on_input, activation)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mmoe_quant_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"intermediate_size_full\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_method\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmoe_quant_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     def _load_per_tensor_weight_scale(self, shard_id: str,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\u001b[0m in \u001b[0;36mcreate_weights\u001b[0;34m(self, layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype, **extra_weight_attrs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# down_proj (row parallel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         w2_weight = torch.nn.Parameter(torch.empty(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mnum_experts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 35483 has 13.69 GiB memory in use. Of the allocated memory 13.57 GiB is allocated by PyTorch, and 8.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(model=\"Qwen/Qwen3-235B-A22B\", dtype=\"half\", max_num_batched_tokens=256)\n"
      ],
      "metadata": {
        "id": "99MSGvOj8c6g",
        "outputId": "9f6877e2-0f63-4344-9afe-23e9662804c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 05-08 12:49:58 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 05-08 12:49:58 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\n",
            "WARNING 05-08 12:49:58 [arg_utils.py:1525] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.\n",
            "INFO 05-08 12:49:58 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=256.\n",
            "INFO 05-08 12:49:58 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='Qwen/Qwen3-235B-A22B', speculative_config=None, tokenizer='Qwen/Qwen3-235B-A22B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen3-235B-A22B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "INFO 05-08 12:50:01 [model_runner.py:1108] Starting to load model Qwen/Qwen3-235B-A22B...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 35483 has 13.69 GiB memory in use. Of the allocated memory 13.57 GiB is allocated by PyTorch, and 8.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-cae18198b293>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Qwen/Qwen3-235B-A22B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"half\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_batched_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     )\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Create the Engine (autoselects V0 vs V1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    248\u001b[0m             engine_args=engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_engine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mengine_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV1LLMEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         return engine_cls.from_vllm_config(\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0musage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musage_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_vllm_config\u001b[0;34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mdisable_log_stats\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     ) -> \"LLMEngine\":\n\u001b[0;32m--> 486\u001b[0;31m         return cls(\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_executor_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                                                     mm_registry)\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"pooling\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sleeping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleeping_tags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\u001b[0m in \u001b[0;36m_init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollective_rpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_worker\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollective_rpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_device\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollective_rpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"load_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     def collective_rpc(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\u001b[0m in \u001b[0;36mcollective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2454\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2456\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     def save_sharded_state(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mDeviceMemoryProfiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mtime_before_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m                 assert supports_lora(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/__init__.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(vllm_config)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVllmConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_default_torch_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtarget_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_initialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0mweights_to_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36m_initialize_model\u001b[0;34m(vllm_config, prefix, model_class)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# new-style model class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_current_vllm_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_compile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     msg = (\"vLLM model class should accept `vllm_config` and `prefix` as \"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         self.model = Qwen3MoeModel(vllm_config=vllm_config,\n\u001b[0m\u001b[1;32m    489\u001b[0m                                    prefix=maybe_prefix(prefix, \"model\"))\n\u001b[1;32m    490\u001b[0m         self.lm_head = ParallelLMHead(config.vocab_size,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/compilation/decorators.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVllmConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mold_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvllm_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# for CompilationLevel.DYNAMO_AS_IS , the upper level model runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             prefix=f\"{prefix}.embed_tokens\")\n\u001b[0;32m--> 334\u001b[0;31m         self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             lambda prefix: Qwen3MoeDecoderLayer(config=config,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\u001b[0m in \u001b[0;36mmake_layers\u001b[0;34m(num_hidden_layers, layer_fn, prefix)\u001b[0m\n\u001b[1;32m    607\u001b[0m                                             get_pp_group().world_size)\n\u001b[1;32m    608\u001b[0m     modules = torch.nn.ModuleList(\n\u001b[0;32m--> 609\u001b[0;31m         [PPMissingLayer() for _ in range(start_layer)] + [\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mmaybe_offload_to_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{prefix}.{idx}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    608\u001b[0m     modules = torch.nn.ModuleList(\n\u001b[1;32m    609\u001b[0m         [PPMissingLayer() for _ in range(start_layer)] + [\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0mmaybe_offload_to_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{prefix}.{idx}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         ] + [PPMissingLayer() for _ in range(end_layer, num_hidden_layers)])\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m    334\u001b[0m         self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[1;32m    335\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             lambda prefix: Qwen3MoeDecoderLayer(config=config,\n\u001b[0m\u001b[1;32m    337\u001b[0m                                                 \u001b[0mcache_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                                                 \u001b[0mquant_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, cache_config, quant_config, prefix)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_experts\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             (layer_idx + 1) % config.decoder_sparse_step == 0):\n\u001b[0;32m--> 278\u001b[0;31m             self.mlp = Qwen3MoeSparseMoeBlock(config=config,\n\u001b[0m\u001b[1;32m    279\u001b[0m                                               \u001b[0mquant_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                                               prefix=f\"{prefix}.mlp\")\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen3_moe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, quant_config, prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 f\"the number of experts {config.num_experts}.\")\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         self.experts = FusedMoE(num_experts=config.num_experts,\n\u001b[0m\u001b[1;32m    114\u001b[0m                                 \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_experts_per_tok\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                                 \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_experts, top_k, hidden_size, intermediate_size, params_dtype, reduce_results, renormalize, use_grouped_topk, num_expert_group, topk_group, quant_config, tp_size, ep_size, dp_size, prefix, custom_routing_function, scoring_func, e_score_correction_bias, apply_router_weight_on_input, activation)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mmoe_quant_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"intermediate_size_full\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_method\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmoe_quant_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     def _load_per_tensor_weight_scale(self, shard_id: str,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\u001b[0m in \u001b[0;36mcreate_weights\u001b[0;34m(self, layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype, **extra_weight_attrs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# down_proj (row parallel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         w2_weight = torch.nn.Parameter(torch.empty(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mnum_experts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.05 GiB is free. Process 35483 has 13.69 GiB memory in use. Of the allocated memory 13.57 GiB is allocated by PyTorch, and 8.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(model=\"Qwen/Qwen3-235B-A22B\", dtype=\"half\", tensor_parallel_size=2)\n"
      ],
      "metadata": {
        "id": "Lzx0IGB78mi7",
        "outputId": "9a16dd56-413a-432c-f5db-ff2754d3e2f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 05-08 12:50:24 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 05-08 12:50:24 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\n",
            "WARNING 05-08 12:50:24 [arg_utils.py:1525] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.\n",
            "INFO 05-08 12:50:24 [config.py:1770] Defaulting to use ray for distributed inference\n",
            "INFO 05-08 12:50:24 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "WARNING 05-08 12:50:24 [config.py:1443] Possibly too large swap space. 8.00 GiB out of the 12.67 GiB total CPU memory is allocated for the swap space.\n",
            "INFO 05-08 12:50:24 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='Qwen/Qwen3-235B-A22B', speculative_config=None, tokenizer='Qwen/Qwen3-235B-A22B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen3-235B-A22B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-08 12:50:28,122\tINFO worker.py:1888 -- Started a local Ray instance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 05-08 12:50:29 [ray_utils.py:335] No current placement group found. Creating a new placement group.\n",
            "WARNING 05-08 12:50:29 [ray_utils.py:342] The number of required GPUs exceeds the total number of available GPUs in the placement group.\n",
            "INFO 05-08 12:50:39 [ray_utils.py:233] Waiting for creating a placement group of specs for 10 seconds. specs=[{'node:172.28.0.12': 0.001, 'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.\n",
            "INFO 05-08 12:50:59 [ray_utils.py:233] Waiting for creating a placement group of specs for 30 seconds. specs=[{'node:172.28.0.12': 0.001, 'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.\n",
            "INFO 05-08 12:51:39 [ray_utils.py:233] Waiting for creating a placement group of specs for 70 seconds. specs=[{'node:172.28.0.12': 0.001, 'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.\n",
            "INFO 05-08 12:52:59 [ray_utils.py:233] Waiting for creating a placement group of specs for 150 seconds. specs=[{'node:172.28.0.12': 0.001, 'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"https://your-api-endpoint/v1/chat/completions\"  # Replace with real API\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "payload = {\n",
        "    \"model\": \"Qwen/Qwen3-235B-A22B\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "id": "uryjsHT-4ot1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1uc_pN4-4oru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nQ6b8qCn4opj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ujOV7q3C4onb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "# Step 1: Install vLLM using pip\n",
        "subprocess.run([\"pip\", \"install\", \"vllm\"])\n",
        "\n",
        "# Step 2: Start the vLLM server\n",
        "subprocess.run([\"vllm\", \"serve\", \"Qwen/Qwen3-235B-A22B\"])\n",
        "\n",
        "# Step 3: Define the API endpoint and request payload\n",
        "url = \"http://localhost:8000/v1/chat/completions\"\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "payload = {\n",
        "    \"model\": \"Qwen/Qwen3-235B-A22B\",\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the capital of France?\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Step 4: Make the request and get the response\n",
        "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "\n",
        "# Step 5: Print the response\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "id": "0Rk7Bx2J3K-V",
        "outputId": "2d3fbd22-d994-47a1-815f-51db17e5e43e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionError",
          "evalue": "HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a291f91ced0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    199\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             conn.request(\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             raise NewConnectionError(\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Failed to establish a new connection: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7a291f91ced0>: Failed to establish a new connection: [Errno 111] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    842\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreason\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a291f91ced0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-df209258ec94>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Step 4: Make the request and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Step 5: Print the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a291f91ced0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Load the trained model\n",
        "model = joblib.load('/content/chatbot_model.pkl')  # Adjust path if needed\n",
        "\n",
        "# User input for prediction\n",
        "year, month, day, hour, minute = 2025, 6, 10, 12, 30  # Example input\n",
        "\n",
        "# Prepare input data\n",
        "input_data = pd.DataFrame([[year, month, day, hour, minute]], columns=['Year', 'Month', 'Day', 'Hour', 'Minute'])\n",
        "\n",
        "# Get prediction\n",
        "prediction = model.predict(input_data)[0]\n",
        "status = \"Overflow\" if prediction >= 1 else \"Normal\"\n",
        "\n",
        "# Print result\n",
        "print(f\"Predicted Max Total: {prediction:.3f}, Status: {status}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "5dvJmftYn7s3",
        "outputId": "cb0d3e6b-38e1-464a-ade9-3bbdc9402acf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/chatbot_model.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d74486f82eef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/chatbot_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust path if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# User input for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/chatbot_model.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aF_DfBW2n7qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4RtMs6vwn7n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iEvirVCTn7lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ahYHSR7sn7jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L71NZpy5n7gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Tt2bATen7dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zdtfTds-n7bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TYXXoCa8n7ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z6K8hWb9n7Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cg6ub8rfn7Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FUZTQPurn7SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P3NLVS-yn7Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MklauKkkn7Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.to_csv('/content/grouped_5_intervals_updated.csv', index=False)\n",
        "result_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zKq5RqXFEEo0",
        "outputId": "4e0cfcb2-fef4-46ad-a4d4-c2b5ff95aa9c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Year  Month  Day      Time      Interval Start  Max Total  status\n",
              "0  2023     10    3  00:00:00 2023-10-03 00:00:00   0.114179  Normal\n",
              "1  2023     10    3  00:02:00 2023-10-03 00:02:00   0.114179  Normal\n",
              "2  2023     10    3  00:04:00 2023-10-03 00:04:00   0.117965  Normal\n",
              "3  2023     10    3  00:06:00 2023-10-03 00:06:00   0.117971  Normal\n",
              "4  2023     10    3  00:08:00 2023-10-03 00:08:00   0.117977  Normal"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4bb522a1-9332-4515-a4f2-0abefecfbc3a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Day</th>\n",
              "      <th>Time</th>\n",
              "      <th>Interval Start</th>\n",
              "      <th>Max Total</th>\n",
              "      <th>status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>00:00:00</td>\n",
              "      <td>2023-10-03 00:00:00</td>\n",
              "      <td>0.114179</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>00:02:00</td>\n",
              "      <td>2023-10-03 00:02:00</td>\n",
              "      <td>0.114179</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>00:04:00</td>\n",
              "      <td>2023-10-03 00:04:00</td>\n",
              "      <td>0.117965</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>00:06:00</td>\n",
              "      <td>2023-10-03 00:06:00</td>\n",
              "      <td>0.117971</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>00:08:00</td>\n",
              "      <td>2023-10-03 00:08:00</td>\n",
              "      <td>0.117977</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4bb522a1-9332-4515-a4f2-0abefecfbc3a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4bb522a1-9332-4515-a4f2-0abefecfbc3a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4bb522a1-9332-4515-a4f2-0abefecfbc3a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e1a5ea7b-8e3a-48f2-9c90-a0d7f656c9d4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e1a5ea7b-8e3a-48f2-9c90-a0d7f656c9d4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e1a5ea7b-8e3a-48f2-9c90-a0d7f656c9d4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "result_df"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, confusion_matrix"
      ],
      "metadata": {
        "id": "GxrPdx5kBPZC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/grouped_5_intervals.csv', parse_dates=['Interval Start'], dayfirst=True)\n",
        "\n",
        "# Prepare the data\n",
        "df.set_index('Interval Start', inplace=True)\n",
        "\n",
        "# Check for NaN values in the 'Max_total' column and handle them\n",
        "# For example, you can fill NaN values with the mean:\n",
        "df['Max Total'].fillna(df['Max Total'].mean(), inplace=True)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_size = int(len(df) * 0.8)\n",
        "train, test = df.iloc[:train_size], df.iloc[train_size:]\n",
        "\n",
        "# Fit the ARIMA model\n",
        "model = ARIMA(train['Max Total'], order=(5,1,0))\n",
        "model_fit = model.fit()\n",
        "\n",
        "# Make predictions\n",
        "predictions = model_fit.forecast(steps=len(test))\n",
        "test['Predicted'] = predictions\n",
        "\n",
        "# Check for NaN values in the 'Predicted' column and handle them (if necessary)\n",
        "df.dropna(inplace=True)\n",
        "# Calculate performance metrics\n",
        "mse = mean_squared_error(test['Max Total'], test['Predicted'])\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Add status column\n",
        "test['Status'] = np.where(test['Predicted'] >= 1, 'Overflow', 'Normal')\n",
        "\n",
        "# Calculate accuracy and F1 score\n",
        "test['Actual_Status'] = np.where(test['Max Total'] >= 1, 'Overflow', 'Normal')\n",
        "accuracy = accuracy_score(test['Actual_Status'], test['Status'])\n",
        "f1 = f1_score(test['Actual_Status'], test['Status'], pos_label='Overflow')\n",
        "\n",
        "# Print results\n",
        "print(f'MSE: {mse}')\n",
        "print(f'RMSE: {rmse}')\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQoOPOZS93ea",
        "outputId": "8e74a3af-ef4c-4130-a689-aaf558c6c3e8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-bed427839d9a>:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Max Total'].fillna(df['Max Total'].mean(), inplace=True)\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency 5min will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency 5min will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency 5min will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "<ipython-input-45-bed427839d9a>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test['Predicted'] = predictions\n",
            "<ipython-input-45-bed427839d9a>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test['Status'] = np.where(test['Predicted'] >= 1, 'Overflow', 'Normal')\n",
            "<ipython-input-45-bed427839d9a>:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test['Actual_Status'] = np.where(test['Max Total'] >= 1, 'Overflow', 'Normal')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.031278222416460895\n",
            "RMSE: 0.1768565023301685\n",
            "Accuracy: 99.34%\n",
            "F1 Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HaAyX_5K93Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ML0ZW_9X93XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LYBBtcOz93Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WP4thC-L93SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yNpFlAB693Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RmzE8mC993EQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}